{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: Dim_Customers\n",
      "    customer_id  customer_zip_code_prefix       customer_city customer_state\n",
      "0  hCT0x9JiGXBQ                     58125     varzea paulista             SP\n",
      "1  PxA7fv9spyhx                      3112  armacao dos buzios             RJ\n",
      "2  g3nXeJkGI0Qw                      4119             jandira             SP\n",
      "3  EOEsCQ6QlpIg                     18212          uberlandia             MG\n",
      "4  mVz5LO2Vd6cL                     88868            ilhabela             SP\n",
      "Loaded file: Dim_OrderItems\n",
      "       order_id    product_id     seller_id   price  shipping_charges\n",
      "0  Axfy13Hk4PIk  90K0C1fIyQUf  ZWM05J9LcBSF  223.51             84.65\n",
      "1  v6px92oS8cLG  qejhpMGGVcsl  IjlpYfhUbRQs  170.80             23.79\n",
      "2  Ulpf9skrhjfm  qUS5d2pEAyxJ  77p2EYxcM9MD   64.40             17.38\n",
      "3  bwJVWupf2keN  639iGvMyv0De  jWzS0ayv9TGf  264.50             30.72\n",
      "4  Dd0QnrMk9Cj5  1lycYGcsic2F  l1pYW6GBnPMr  779.90             30.66\n",
      "Loaded file: Dim_Payments\n",
      "       order_id  payment_sequential payment_type  payment_installments  \\\n",
      "0  Axfy13Hk4PIk                   1  credit_card                     1   \n",
      "1  v6px92oS8cLG                   1  credit_card                     8   \n",
      "2  Ulpf9skrhjfm                   1  credit_card                     4   \n",
      "3  bwJVWupf2keN                   1  credit_card                     2   \n",
      "4  Dd0QnrMk9Cj5                   1  credit_card                     1   \n",
      "\n",
      "   payment_value  \n",
      "0         259.14  \n",
      "1         382.39  \n",
      "2         249.25  \n",
      "3          27.79  \n",
      "4          76.15  \n",
      "Loaded file: Dim_Products\n",
      "     product_id     product_category_name  product_weight_g  \\\n",
      "0  90K0C1fIyQUf                      toys             491.0   \n",
      "1  qejhpMGGVcsl             watches_gifts             440.0   \n",
      "2  qUS5d2pEAyxJ  costruction_tools_garden            2200.0   \n",
      "3  639iGvMyv0De                      toys            1450.0   \n",
      "4  1lycYGcsic2F                      toys             300.0   \n",
      "\n",
      "   product_length_cm  product_height_cm  product_width_cm  \n",
      "0               19.0               12.0              16.0  \n",
      "1               18.0               14.0              17.0  \n",
      "2               16.0               16.0              16.0  \n",
      "3               68.0                3.0              48.0  \n",
      "4               17.0                4.0              12.0  \n",
      "Loaded file: Fact_Orders\n",
      "       order_id   customer_id order_status order_purchase_timestamp  \\\n",
      "0  Axfy13Hk4PIk  hCT0x9JiGXBQ    delivered      2017-10-22 18:57:54   \n",
      "1  v6px92oS8cLG  PxA7fv9spyhx    delivered      2018-06-20 21:40:31   \n",
      "2  Ulpf9skrhjfm  g3nXeJkGI0Qw    delivered      2018-02-16 16:19:31   \n",
      "3  bwJVWupf2keN  EOEsCQ6QlpIg    delivered      2018-08-18 18:04:29   \n",
      "4  Dd0QnrMk9Cj5  mVz5LO2Vd6cL    delivered      2017-12-22 16:44:04   \n",
      "\n",
      "     order_approved_at order_delivered_timestamp order_estimated_delivery_date  \n",
      "0  2017-10-22 19:14:13       2017-10-26 22:19:52                    2017-11-09  \n",
      "1  2018-06-20 22:20:20       2018-07-03 22:51:22                    2018-07-24  \n",
      "2  2018-02-17 16:15:35       2018-02-27 01:29:50                    2018-03-08  \n",
      "3  2018-08-18 18:15:16       2018-08-27 20:03:51                    2018-09-19  \n",
      "4  2017-12-22 17:31:31       2018-01-05 19:22:49                    2018-01-18  \n"
     ]
    }
   ],
   "source": [
    "# Define the folder path containing your CSV files\n",
    "folder_path = r\"C:\\Users\\sande\\OneDrive\\Desktop\\power BI\\Supply Chain Anomaly Detection\\Ecommerce Order Dataset\\train\"\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Load each file into a separate DataFrame\n",
    "for file in csv_files:\n",
    "    file_name = os.path.splitext(file)[0]  # Remove the .csv extension\n",
    "    file_path = os.path.join(folder_path, file)  # Full path to the file\n",
    "    globals()[file_name] = pd.read_csv(file_path)  # Create a variable dynamically\n",
    "\n",
    "    # Display confirmation and preview\n",
    "    print(f\"Loaded file: {file_name}\")\n",
    "    print(globals()[file_name].head())  # Preview the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dim_Products\n",
    "# Dim_Payments\n",
    "# Dim_OrderItems\n",
    "# Dim_Customers\n",
    "# Fact_Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_id               0\n",
      "product_category_name    0\n",
      "product_weight_g         0\n",
      "product_length_cm        0\n",
      "product_height_cm        0\n",
      "product_width_cm         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_11988\\282907363.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  Dim_Products['product_category_name'].fillna('Unknown', inplace=True)\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_11988\\282907363.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  Dim_Products[col].fillna(Dim_Products[col].median(), inplace=True)\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_11988\\282907363.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  Dim_Products[col].fillna(Dim_Products[col].median(), inplace=True)\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_11988\\282907363.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  Dim_Products[col].fillna(Dim_Products[col].median(), inplace=True)\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_11988\\282907363.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  Dim_Products[col].fillna(Dim_Products[col].median(), inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.to_csv of          product_id     product_category_name  product_weight_g  \\\n",
       "0      90K0C1fIyQUf                      toys             491.0   \n",
       "1      qejhpMGGVcsl             watches_gifts             440.0   \n",
       "2      qUS5d2pEAyxJ  costruction_tools_garden            2200.0   \n",
       "3      639iGvMyv0De                      toys            1450.0   \n",
       "4      1lycYGcsic2F                      toys             300.0   \n",
       "...             ...                       ...               ...   \n",
       "89311  W8vikEizUggJ                      toys             700.0   \n",
       "89312  KXSbyJWtMMwZ                      toys             600.0   \n",
       "89313  EG4wDSpFyTth             health_beauty             250.0   \n",
       "89314  ZWyg4uNWPHjJ                      toys             250.0   \n",
       "89315  5pbzE0rDy61l                      toys            1750.0   \n",
       "\n",
       "       product_length_cm  product_height_cm  product_width_cm  \n",
       "0                   19.0               12.0              16.0  \n",
       "1                   18.0               14.0              17.0  \n",
       "2                   16.0               16.0              16.0  \n",
       "3                   68.0                3.0              48.0  \n",
       "4                   17.0                4.0              12.0  \n",
       "...                  ...                ...               ...  \n",
       "89311               21.0               14.0              14.0  \n",
       "89312               16.0               16.0              16.0  \n",
       "89313               22.0               10.0              18.0  \n",
       "89314               16.0                2.0              11.0  \n",
       "89315               32.0               18.0              24.0  \n",
       "\n",
       "[89316 rows x 6 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dim_Products.isnull().sum()\n",
    "# Filling missing values in 'product_category_name' with 'Unknown'\n",
    "Dim_Products['product_category_name'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Filling missing values in numeric columns with their median values\n",
    "Dim_Products.isnull().sum()\n",
    "# Filling missing values in numeric columns with their median values\n",
    "numeric_columns = ['product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']\n",
    "for col in numeric_columns:\n",
    "    Dim_Products[col].fillna(Dim_Products[col].median(), inplace=True)\n",
    "# Check to ensure all missing values are handled\n",
    "print(Dim_Products.isnull().sum())\n",
    "\n",
    "#save modified csv file\n",
    "Dim_Products.to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89316 entries, 0 to 89315\n",
      "Data columns (total 6 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   product_id             89316 non-null  object \n",
      " 1   product_category_name  89316 non-null  object \n",
      " 2   product_weight_g       89316 non-null  float64\n",
      " 3   product_length_cm      89316 non-null  float64\n",
      " 4   product_height_cm      89316 non-null  float64\n",
      " 5   product_width_cm       89316 non-null  float64\n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "Dim_Products.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dim_Payments.isnull().sum()\n",
    "Dim_Payments.duplicated().sum()\n",
    "# Dim_Pyaments are not having any null values or and duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89316 entries, 0 to 89315\n",
      "Data columns (total 5 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   order_id              89316 non-null  object \n",
      " 1   payment_sequential    89316 non-null  int64  \n",
      " 2   payment_type          89316 non-null  object \n",
      " 3   payment_installments  89316 non-null  int64  \n",
      " 4   payment_value         89316 non-null  float64\n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "Dim_Payments.info()\n",
    "# the datatypes are also good and no need of modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89316 entries, 0 to 89315\n",
      "Data columns (total 4 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   customer_id               89316 non-null  object\n",
      " 1   customer_zip_code_prefix  89316 non-null  int64 \n",
      " 2   customer_city             89316 non-null  object\n",
      " 3   customer_state            89316 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "Dim_Customers.isnull().sum()\n",
    "Dim_Customers.duplicated().sum()\n",
    "Dim_Customers.info()\n",
    "# the table is not having any missing values and no need to change the data types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89316 entries, 0 to 89315\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   order_id          89316 non-null  object \n",
      " 1   product_id        89316 non-null  object \n",
      " 2   seller_id         89316 non-null  object \n",
      " 3   price             89316 non-null  float64\n",
      " 4   shipping_charges  89316 non-null  float64\n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "Dim_OrderItems.isnull().sum()\n",
    "Dim_OrderItems.duplicated().sum()\n",
    "Dim_OrderItems.info()\n",
    "# the datatype is good and  no missing values and convienent datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89316 entries, 0 to 89315\n",
      "Data columns (total 7 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   order_id                       89316 non-null  object\n",
      " 1   customer_id                    89316 non-null  object\n",
      " 2   order_status                   89316 non-null  object\n",
      " 3   order_purchase_timestamp       89316 non-null  object\n",
      " 4   order_approved_at              89307 non-null  object\n",
      " 5   order_delivered_timestamp      87427 non-null  object\n",
      " 6   order_estimated_delivery_date  89316 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "Fact_Orders.info()\n",
    "\n",
    "# Convert columns to datetime\n",
    "Fact_Orders['order_purchase_timestamp'] = pd.to_datetime(Fact_Orders['order_purchase_timestamp'])\n",
    "\n",
    "Fact_Orders['order_approved_at'] = pd.to_datetime(Fact_Orders['order_approved_at'])\n",
    "\n",
    "Fact_Orders['order_delivered_timestamp'] = pd.to_datetime(Fact_Orders['order_delivered_timestamp'])\n",
    "\n",
    "Fact_Orders['order_estimated_delivery_date'] = pd.to_datetime(Fact_Orders['order_estimated_delivery_date'])\n",
    "\n",
    "# Create the 'Delay' feature\n",
    "Fact_Orders['delivery_to_estimated_delivery_delay'] = (Fact_Orders['order_delivered_timestamp'] - Fact_Orders['order_estimated_delivery_date']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_timestamp</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "      <th>delivery_to_estimated_delivery_delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>P5R6jr1qZdh4</td>\n",
       "      <td>FrEvnEiMKGpr</td>\n",
       "      <td>canceled</td>\n",
       "      <td>2017-07-24 11:38:43</td>\n",
       "      <td>2017-07-24 11:50:18</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-08-07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>C21fWds5zL0W</td>\n",
       "      <td>iFsAJRrzVaTS</td>\n",
       "      <td>shipped</td>\n",
       "      <td>2017-02-04 12:58:55</td>\n",
       "      <td>2017-02-04 13:10:38</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-03-15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>vSug5AqRo9gE</td>\n",
       "      <td>oWBBqGwqpx4m</td>\n",
       "      <td>shipped</td>\n",
       "      <td>2017-05-07 10:22:58</td>\n",
       "      <td>2017-05-09 09:35:20</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-06-12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2HQ26ShSPhCA</td>\n",
       "      <td>uvuFFZDOAlU7</td>\n",
       "      <td>canceled</td>\n",
       "      <td>2017-07-29 12:56:17</td>\n",
       "      <td>2017-07-29 13:05:18</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1By8LOosrvF2</td>\n",
       "      <td>grsbZ5L1H5ty</td>\n",
       "      <td>canceled</td>\n",
       "      <td>2017-11-06 15:47:20</td>\n",
       "      <td>2017-11-07 07:30:29</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89196</th>\n",
       "      <td>VP3EBMExdh7K</td>\n",
       "      <td>acB2XNtto2SX</td>\n",
       "      <td>shipped</td>\n",
       "      <td>2018-01-24 18:19:18</td>\n",
       "      <td>2018-01-24 18:37:41</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89226</th>\n",
       "      <td>0jPFvDPItbRN</td>\n",
       "      <td>Hb5m8q1YZEfv</td>\n",
       "      <td>canceled</td>\n",
       "      <td>2017-08-02 14:08:18</td>\n",
       "      <td>2017-08-02 14:23:13</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-08-24</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89234</th>\n",
       "      <td>QzxMjvdMpOEI</td>\n",
       "      <td>yCbzy1m4tTUA</td>\n",
       "      <td>shipped</td>\n",
       "      <td>2017-09-15 09:05:41</td>\n",
       "      <td>2017-09-15 09:15:13</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2017-10-05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89262</th>\n",
       "      <td>rnnNWGgyHsA5</td>\n",
       "      <td>6LYRxLvrUzNy</td>\n",
       "      <td>canceled</td>\n",
       "      <td>2017-12-13 12:55:38</td>\n",
       "      <td>2017-12-13 13:11:58</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89281</th>\n",
       "      <td>YCJoeAvBWkpq</td>\n",
       "      <td>BaZgf4zWhXSS</td>\n",
       "      <td>shipped</td>\n",
       "      <td>2018-03-06 11:13:39</td>\n",
       "      <td>2018-03-06 11:30:38</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2018-03-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1889 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           order_id   customer_id order_status order_purchase_timestamp  \\\n",
       "15     P5R6jr1qZdh4  FrEvnEiMKGpr     canceled      2017-07-24 11:38:43   \n",
       "41     C21fWds5zL0W  iFsAJRrzVaTS      shipped      2017-02-04 12:58:55   \n",
       "86     vSug5AqRo9gE  oWBBqGwqpx4m      shipped      2017-05-07 10:22:58   \n",
       "89     2HQ26ShSPhCA  uvuFFZDOAlU7     canceled      2017-07-29 12:56:17   \n",
       "133    1By8LOosrvF2  grsbZ5L1H5ty     canceled      2017-11-06 15:47:20   \n",
       "...             ...           ...          ...                      ...   \n",
       "89196  VP3EBMExdh7K  acB2XNtto2SX      shipped      2018-01-24 18:19:18   \n",
       "89226  0jPFvDPItbRN  Hb5m8q1YZEfv     canceled      2017-08-02 14:08:18   \n",
       "89234  QzxMjvdMpOEI  yCbzy1m4tTUA      shipped      2017-09-15 09:05:41   \n",
       "89262  rnnNWGgyHsA5  6LYRxLvrUzNy     canceled      2017-12-13 12:55:38   \n",
       "89281  YCJoeAvBWkpq  BaZgf4zWhXSS      shipped      2018-03-06 11:13:39   \n",
       "\n",
       "        order_approved_at order_delivered_timestamp  \\\n",
       "15    2017-07-24 11:50:18                       NaT   \n",
       "41    2017-02-04 13:10:38                       NaT   \n",
       "86    2017-05-09 09:35:20                       NaT   \n",
       "89    2017-07-29 13:05:18                       NaT   \n",
       "133   2017-11-07 07:30:29                       NaT   \n",
       "...                   ...                       ...   \n",
       "89196 2018-01-24 18:37:41                       NaT   \n",
       "89226 2017-08-02 14:23:13                       NaT   \n",
       "89234 2017-09-15 09:15:13                       NaT   \n",
       "89262 2017-12-13 13:11:58                       NaT   \n",
       "89281 2018-03-06 11:30:38                       NaT   \n",
       "\n",
       "      order_estimated_delivery_date  delivery_to_estimated_delivery_delay  \n",
       "15                       2017-08-07                                   NaN  \n",
       "41                       2017-03-15                                   NaN  \n",
       "86                       2017-06-12                                   NaN  \n",
       "89                       2017-08-18                                   NaN  \n",
       "133                      2017-11-28                                   NaN  \n",
       "...                             ...                                   ...  \n",
       "89196                    2018-03-01                                   NaN  \n",
       "89226                    2017-08-24                                   NaN  \n",
       "89234                    2017-10-05                                   NaN  \n",
       "89262                    2018-01-02                                   NaN  \n",
       "89281                    2018-03-16                                   NaN  \n",
       "\n",
       "[1889 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fact_Orders[Fact_Orders['order_delivered_timestamp'].isnull()]\n",
    "\n",
    "\n",
    "\n",
    "# lots of null values are there but we cannot replace or change then with strings or dates for cancelled orders its better leave as null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fact_Orders.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file: C:\\Users\\sande\\OneDrive\\Desktop\\power BI\\Supply Chain Anomaly Detection\\Ecommerce Order Dataset\\Filtered-data\\Dim_Customers_filtered.csv\n",
      "Saved file: C:\\Users\\sande\\OneDrive\\Desktop\\power BI\\Supply Chain Anomaly Detection\\Ecommerce Order Dataset\\Filtered-data\\Dim_OrderItems_filtered.csv\n",
      "Saved file: C:\\Users\\sande\\OneDrive\\Desktop\\power BI\\Supply Chain Anomaly Detection\\Ecommerce Order Dataset\\Filtered-data\\Dim_Payments_filtered.csv\n",
      "Saved file: C:\\Users\\sande\\OneDrive\\Desktop\\power BI\\Supply Chain Anomaly Detection\\Ecommerce Order Dataset\\Filtered-data\\Dim_Products_filtered.csv\n",
      "Saved file: C:\\Users\\sande\\OneDrive\\Desktop\\power BI\\Supply Chain Anomaly Detection\\Ecommerce Order Dataset\\Filtered-data\\Fact_Orders_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the output folder path\n",
    "output_folder = r\"C:\\Users\\sande\\OneDrive\\Desktop\\power BI\\Supply Chain Anomaly Detection\\Ecommerce Order Dataset\\Filtered-data\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save each dynamically created DataFrame\n",
    "for file in csv_files:\n",
    "    file_name = os.path.splitext(file)[0]  # Extract the base file name\n",
    "    output_file_path = os.path.join(output_folder, f\"{file_name}_filtered.csv\")  # Create output file path\n",
    "    \n",
    "    # Check if the variable exists in globals and is a DataFrame\n",
    "    if file_name in globals() and isinstance(globals()[file_name], pd.DataFrame):\n",
    "        globals()[file_name].to_csv(output_file_path, index=False)  # Save as CSV without the index\n",
    "        print(f\"Saved file: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
